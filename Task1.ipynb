{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "86/86 [==============================] - 23s 103ms/step - loss: 2.3195 - accuracy: 0.2607 - val_loss: 1.5593 - val_accuracy: 0.6926\n",
      "Epoch 2/5\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1305 - accuracy: 0.6971 - val_loss: 0.7499 - val_accuracy: 0.7900\n",
      "Epoch 3/5\n",
      "86/86 [==============================] - 7s 79ms/step - loss: 0.5920 - accuracy: 0.8355 - val_loss: 0.6128 - val_accuracy: 0.8140\n",
      "Epoch 4/5\n",
      "86/86 [==============================] - 8s 87ms/step - loss: 0.3485 - accuracy: 0.9057 - val_loss: 0.5722 - val_accuracy: 0.8234\n",
      "Epoch 5/5\n",
      "86/86 [==============================] - 6s 68ms/step - loss: 0.2098 - accuracy: 0.9509 - val_loss: 0.5677 - val_accuracy: 0.8249\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 0.5677 - accuracy: 0.8249\n",
      "Test Accuracy: 82.49%\n",
      "1/1 [==============================] - 0s 403ms/step\n",
      "{\"text\": \"With the presentation of the interim budget on February 1\", \"category\": \"entertainment\"}\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "{\"text\": \"Desai, as finance minister, had presented five annual budgets and one interim budget between 1959-1964\", \"category\": \"arts  culture\"}\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "{\"text\": \"ATMs to become virtual bank branches, accept deposits with instant credit\", \"category\": \"business\"}\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "{\"text\": \"Allan Border opens up about Parkinson\\u2019s disease: \\u2018I\\u2019m not scared, but I am worried about the slow decline process\\u2019\\\",\\\"Former Australia captain says the disease has softened him a bit, which is embarrassing and good at the same time.\", \"category\": \"comedy\"}\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "{\"text\": \"Unlocking the science of E Ink displays: Why we believe they must catch on\", \"category\": \"science\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# loading the data\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "# remove punctuations\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, \"\")  \n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # remove digits\n",
    "    return text\n",
    "\n",
    "def combine_text_columns(df, text_columns):\n",
    "    combined_texts = df[text_columns[0]].astype(str)\n",
    "    for col in text_columns[1:]:\n",
    "        combined_texts += \" \" + df[col].astype(str)\n",
    "    return combined_texts\n",
    " # Vectorizing the data and also Adjusting the max_features as needed\n",
    "def vectorize_texts(texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=10000) \n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X, vectorizer\n",
    "# Model for News Article Categorization\n",
    "def build_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(input_dim,), activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Add dropout to reduce overfitting\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Add dropout to reduce overfitting\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "def predict_category(model, vectorizer, texts, label_encoder):\n",
    "    X = vectorizer.transform(texts).toarray()\n",
    "    predictions = model.predict(X)\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "    return predicted_labels\n",
    "\n",
    "def main():\n",
    "    # Load the data\n",
    "    df = load_data('Task1/news-article-categories.csv')\n",
    "\n",
    "    # Specify the text columns and label column\n",
    "    text_columns = ['title', 'body']\n",
    "    label_column = 'category' \n",
    "    df['title'] = df[\"title\"].apply(preprocess_text)\n",
    "    # Preprocess the text data\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].apply(preprocess_text)\n",
    "\n",
    "    # Combine the text columns\n",
    "    combined_texts = combine_text_columns(df, text_columns)\n",
    "\n",
    "    # Vectorize the texts using TF-IDF\n",
    "    X, vectorizer = vectorize_texts(combined_texts)\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = df[label_column].apply(preprocess_text) \n",
    "    y = label_encoder.fit_transform(labels)\n",
    "    y = to_categorical(y)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert sparse matrices to dense\n",
    "    X_train = X_train.toarray()\n",
    "    X_test = X_test.toarray()\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_model(X_train.shape[1], y_train.shape[1])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    # CLI for predicting categories\n",
    "    while True:\n",
    "        input_text = input(\"Enter news article text (or type 'exit' to quit): \")\n",
    "        if input_text.lower() == 'exit':\n",
    "            break\n",
    "        predicted_category = predict_category(model, vectorizer, [input_text], label_encoder)\n",
    "        print(json.dumps({'text': input_text, 'category': predicted_category[0]}))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
